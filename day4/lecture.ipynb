{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Please go to https://ccv.jupyter.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we learned so far...\n",
    "- Functions for making code organized and reusable\n",
    "- Comprehensions for constructing sequences from other sequences\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Intro to Natural Language Processing (NLP)\n",
    "### By the end of the day, you'll be able to \n",
    "- describe the main goal of NLP in machine learning\n",
    "- become familiar with text normalization\n",
    "- explain the differences between stemming and lemmatization\n",
    "- design code to pre-process a text corpus for downstream text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Machine Learning and NLP\n",
    "\n",
    "Typical problems you can solve:\n",
    "- *classification and regression* problems like\n",
    "    - grading essays\n",
    "    - predicting stock price based on news articles (kaggle competition) and/or the president's tweets (conference talk)\n",
    "    - predicting the author of an article from a set of authors (e.g., anonymous NYT op-ed)\n",
    "    - predicting the topic of articles from a set of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *unsupervised* problems like    \n",
    "    - group documents based on similarity\n",
    "    - extract topics from a set of documents\n",
    "- other\n",
    "    - sentiment analysis\n",
    "    - summarize a large corpus which is too long to read for a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 The steps in the NLP pipeline\n",
    "1. Text normalization\n",
    "2. Stem or lemmatize the words \n",
    "3. Collect the unique words in the corpus\n",
    "4. Count how many times each unique word appears in the documents (count matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 Step 1: Text Normalization\n",
    "### Can be tricky, spend some time considering the following:\n",
    "- Lower case of all characters is usually a good idea\n",
    "- Do we care about special characters? (social media corpus: yes!!!??? :) ;) :D :( )\n",
    "- Do we care about numbers? (for scientific articles, probably)\n",
    "- Are there non-English words in the corpus?\n",
    "- Do we need to worry about misspelled words? (usually yes, unless you work with classics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Robust and fast functions for text analysis have been implemented by Python developers in the `sklearn` and `nltk` packages. \n",
    "scikit learn:\n",
    "- Simple and efficient tools for machine learning, data mining, and data analysis\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "\n",
    "nltk (Natural Language ToolKit):\n",
    "- Work with human language data\n",
    "- Over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.3.1 Document Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"  The 5 biggest countries by population in 2017 are: China, India, United States, Indonesia, and Brazil. \\\n",
    "Aren't they neat?   \"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = doc.strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "contractions.fix(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.3.2 Tokenization and Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "words = nltk.word_tokenize(doc)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# replace integer occurrences in list of tokenized words with textual representation\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "numbers = [p.number_to_words(word) for word in words if word.isdigit()] \n",
    "words = [word for word in words if word.isdigit() == False]\n",
    "words = numbers + words\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "words = [i for i in words if not i in stopwords]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "## Write a function to normalize a document and apply it to this corpus:\n",
    "\n",
    "```\n",
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 Step 2: Stemming and Lemmatization\n",
    "#### Goal: reduce derived words to a common base.\n",
    "\n",
    "An example list of unique words:\n",
    "- ['time', 'timed', 'timing', 'times']\n",
    "- ['was', 'is' ,'am', 'were']\n",
    "- ['operate', 'operating', 'operates', 'operation', 'operative', 'operatives', 'operational']\n",
    "\n",
    "#### Are these different forms of the same word or different words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.1 Stemming\n",
    "- Stemming is the process of reducing inflection in words to their root forms.\n",
    "- Cut off endings, so sometimes stem is not a valid word in the language. \n",
    "- See Porter, 1980 (An algorithm for suffix stripping. Program 14 (3): 130-137)  for more details.\n",
    "\n",
    "#### Some general rules:\n",
    "- SSES => SS (e.g., dresses => dress)\n",
    "- IES => I (e.g., ponies => poni)\n",
    "- SS => SS (e.g., class => class)\n",
    "- S => _ (e.g., cats => cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['time', 'timed', 'timing', 'times',\\\n",
    "         'was','is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operation', 'operative', 'operatives', 'operational']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w + ' => ' + ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Write a function to stem a document and apply it to `norm_corpus` from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.2 Lemmatization\n",
    "- Lemmatization, unlike Stemming, reduces the inflected words while ensuring that the root word belongs to the language. \n",
    "- A lemma is the dictionary form of a set of semantically similar words.\n",
    "- Can take into account part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time => time\n",
      "timed => timed\n",
      "timing => timing\n",
      "times => time\n",
      "was => wa\n",
      "is => is\n",
      "am => am\n",
      "are => are\n",
      "operate => operate\n",
      "operating => operating\n",
      "operates => operates\n",
      "operations => operation\n",
      "operative => operative\n",
      "operatives => operative\n",
      "operational => operational\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = ['time', 'timed', 'timing', 'times',\\\n",
    "         'was','is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operations', 'operative', 'operatives', 'operational']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in words:\n",
    "    print(w + ' => ' + lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word, pos='n') method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lemmatizer.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Googled \"python nltk lemmatization pos\" and this came up:\n",
    "https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words = ['time', 'timed', 'timing', 'times', 'was', 'is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operations', 'operative', 'operatives', 'operational',\\\n",
    "         'drunk','drunk','saw','saw']\n",
    "pos_tags = ['n','v','v','n', 'v','v','v','v',\\\n",
    "            'v','v','v','n','n','n','a',\\\n",
    "            'v','n','v','n']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w, p in zip(words, pos_tags):\n",
    "    print(w + ' ' + p + ' => ' + lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `nltk` has a part-of-speech tagger that is useful for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "for tw in tagged_words:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# function that converts nltk.pos_tag output to valid values for pos argument in the lemmatize method (wordnet tag)\n",
    "from nltk.corpus.reader import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for word, tag in tagged_words:\n",
    "    wn_tag = get_wordnet_pos(tag)\n",
    "    print(word + ' => ' + wn_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "##  Write a function that POS tags and lemmatizes the documents in `norm_corpus` from Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.3 Pros and cons of stemming and lemmatization\n",
    "- Stemming is faster but less accurate\n",
    "- Lemmatization is slower but it can be more accurate (but usually not by much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 Step 3: Collect the unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmed_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-29e6f4d8f1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmed_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmed_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "lemmed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmed_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e64b62c2163f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# nested set comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munique_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmed_corpus\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0munique_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmed_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# nested set comprehension\n",
    "unique_words = set([word for doc in lemmed_corpus for word in doc])\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.6 Step 4: Count how many times each unique word appears in the documents (count matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The count matrix:\n",
    "|-|character|say|age|bad|best|charles|dickens|foolishness|time|wisdom|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Doc1|1| 1| 0| 0| 1| 0| 0| 0| 1| 0|\n",
    "|Doc2|1| 1| 0| 1| 0| 0| 0| 0| 1| 0|\n",
    "|Doc3|1| 1| 1| 0| 0| 0| 0| 0| 0| 1|\n",
    "|Doc4|0| 0| 1| 0| 0| 1| 1| 1| 0| 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Word order does not matter!\n",
    "|-|age|bad|best|character|charles|dickens|foolishness|say|time|wisdom|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Doc1|0| 0| 1| 1| 0| 0| 0| 1| 1| 0|\n",
    "|Doc2|0| 1| 0| 1| 0| 0| 0| 1| 1| 0|\n",
    "|Doc3|1| 0| 0| 1| 0| 0| 0| 1| 0| 1|\n",
    "|Doc4|1| 0| 0| 0| 1| 1| 1| 0| 0| 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document vectors:\n",
    "- 'The character said: It was the best of times, ' => 'character said best time' => [0,0,1,1,0,0,0,1,1,0]\n",
    "- 'The character said: it was the worst of times, ' => 'character said worst time' => [0,1,0,1,0,0,01,1,0]\n",
    "- 'The character said: it was the Age of Wisdom, ' => 'character said age wisdom' => [1,0,0,1,0,0,0,1,0,1]\n",
    "- 'it was the Age of Foolishness. - Charles Dickens' => 'age foolishness charles dickens' => [1,0,0,0,1,1,1,0,0,0]\n",
    "___\n",
    "Key: ['age', 'bad', 'best', 'character', 'charles', 'dickens', 'foolishness', 'say', 'time', 'wisdom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.1 Bag of Words\n",
    "\n",
    "- A method to extract features from text documents\n",
    "- Can be used for training machine learning algorithms\n",
    "- Creates a vocabulary of unique words occurring in all documents\n",
    "- Represents a document as word counts, disregarding the order in which they appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.2 CountVectorizer will construct a bag of words matrix from a corpus of documents and it supports some of the pre-processing functions we coded before (lowercase, remove punctuation, split into words/tokens, remove stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.3 For more granular control over pre-processing, do it outside of CountVectorizer, join your tokens back together, then run CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lemmed_corpus)\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "print(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "- Main goal of NLP in ML is to convert variable length documents into fixed length numbers\n",
    "- Text Normalization is the process of cleaning and processing raw text\n",
    "- Stemming and lemmatization are two attempts to reduce derived words to their bases\n",
    "- Bag of words counts how many times each unique word appears in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
