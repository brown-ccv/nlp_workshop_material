{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Please go to https://ccv.jupyter.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we learned so far...\n",
    "- Functions for making code organized and reusable\n",
    "- Comprehensions for constructing sequences from other sequences\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Intro to Natural Language Processing (NLP)\n",
    "### By the end of the day, you'll be able to \n",
    "- describe the main goal of NLP in machine learning\n",
    "- become familiar with text normalization\n",
    "- explain the differences between stemming and lemmatization\n",
    "- design code to pre-process a text corpus for downstream text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Machine Learning and NLP\n",
    "\n",
    "Typical problems you can solve:\n",
    "- *classification and regression* problems like\n",
    "    - grading essays\n",
    "    - predicting stock price based on news articles (kaggle competition) and/or the president's tweets (conference talk)\n",
    "    - predicting the author of an article from a set of authors (e.g., anonymous NYT op-ed)\n",
    "    - predicting the topic of articles from a set of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *unsupervised* problems like    \n",
    "    - group documents based on similarity\n",
    "    - extract topics from a set of documents\n",
    "- other\n",
    "    - sentiment analysis\n",
    "    - summarize a large corpus which is too long to read for a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 The steps in the NLP pipeline\n",
    "1. Text normalization\n",
    "2. Stem or lemmatize the words \n",
    "3. Collect the unique words in the corpus\n",
    "4. Count how many times each unique word appears in the documents (count matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 Step 1: Text Normalization\n",
    "### Can be tricky, spend some time considering the following:\n",
    "- Lower case of all characters is usually a good idea\n",
    "- Do we care about special characters? (social media corpus: yes!!!??? :) ;) :D :( )\n",
    "- Do we care about numbers? (for scientific articles, probably)\n",
    "- Are there non-English words in the corpus?\n",
    "- Do we need to worry about misspelled words? (usually yes, unless you work with classics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Robust and fast functions for text analysis have been implemented by Python developers in the `sklearn` and `nltk` packages. \n",
    "scikit learn:\n",
    "- Simple and efficient tools for machine learning, data mining, and data analysis\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "\n",
    "nltk (Natural Language ToolKit):\n",
    "- Work with human language data\n",
    "- Over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.3.1 Document Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"  The 5 biggest countries by population in 2017 are: China, India, United States, Indonesia, and Brazil. \\\n",
    "Aren't they neat?   \"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = doc.strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "contractions.fix(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.3.2 Tokenization and Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "words = nltk.word_tokenize(doc)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# replace integer occurrences in list of tokenized words with textual representation\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "numbers = [p.number_to_words(word) for word in words if word.isdigit()] \n",
    "words = [word for word in words if word.isdigit() == False]\n",
    "words = numbers + words\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "words = [i for i in words if not i in stopwords]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "## Write a function to normalize a document and apply it to this corpus:\n",
    "\n",
    "```\n",
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 Step 2: Stemming and Lemmatization\n",
    "#### Goal: reduce derived words to a common base.\n",
    "\n",
    "An example list of unique words:\n",
    "- ['time', 'timed', 'timing', 'times']\n",
    "- ['was', 'is' ,'am', 'were']\n",
    "- ['operate', 'operating', 'operates', 'operation', 'operative', 'operatives', 'operational']\n",
    "\n",
    "#### Are these different forms of the same word or different words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.1 Example\n",
    "\n",
    "#### Try to return the dictionary form of the word via morphological analysis (not perfect!)\n",
    "Example: 'drunk'\n",
    "- Stemming would give us 'drunk'\n",
    "- Lemmatization would return either 'drink' or 'drunk' depending on whether 'drunk' is likely a verb or noun in the sentence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = ['drunk','drunk']\n",
    "pos_tags = ['n','v']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for w,p in zip(words,pos_tags):\n",
    "    print(w,p,'=>',lemmatizer.lemmatize(w,pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.2 Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language. (See Porter, 1980 (An algorithm for suffix stripping. Program 14 (3): 130-137)  for more details.)\n",
    "\n",
    "#### Some general rules:\n",
    "- SSES => SS (e.g., dresses => dress)\n",
    "- IES => I (e.g., ponies => poni)\n",
    "- SS => SS (e.g., class => class)\n",
    "- S => _ (e.g., cats => cat)\n",
    "- EMENT => _ if the root is longer than 1 (e.g., replacement => replac but cement does not change to c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['time', 'timed', 'timing', 'times',\\\n",
    "         'was','is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operation', 'operative', 'operatives', 'operational']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w + ' => ' + ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Write a function to stem a document and apply it to `norm_corpus` from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem: stem of 'operate', 'operating', 'operates', 'operation', 'operative', 'operatives', 'operational' is _'oper'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.3 Lemmatization\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words = ['time', 'timed', 'timing', 'times',\\\n",
    "         'was','is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operations', 'operative', 'operatives', 'operational']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in words:\n",
    "    print(w + ' => ' + lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "help(lemmatizer.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Googled \"python nltk lemmatization pos\" and this came up:\n",
    "https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words = ['time', 'timed', 'timing', 'times', 'was', 'is','am','are',\\\n",
    "         'operate', 'operating', 'operates', 'operations', 'operative', 'operatives', 'operational',\\\n",
    "         'drunk','drunk','saw','saw']\n",
    "pos_tags = ['n','v','v','n', 'v','v','v','v',\\\n",
    "            'v','v','v','n','n','n','a',\\\n",
    "            'v','n','v','n']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w, p in zip(words, pos_tags):\n",
    "    print(w + ' ' + p + ' => ' + lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `nltk` has a part-of-speech tagger that is useful for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "for tw in tagged_words:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "## Write a function that prepares wordnet POS tags for input to the `lemmatize()` function\n",
    "\n",
    "- Hint 1: There is a string method, `.startswith()`\n",
    "- Hint 2: Use the `None` keyword for POS's other than nouns, verbs, adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "##  Using the function from Exercise 3 to automatically tag POS, write a function that lemmatizes the documents in `norm_corpus` from Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.4 Pros and cons of stemming and lemmatization\n",
    "- Stemming is faster but less accurate\n",
    "- Lemmatization is slower but it can be more accurate (but usually not by much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 Step 3: Collect the unique words in the corpus\n",
    "\n",
    "Can use `set()` or `np.unique()`\n",
    "```\n",
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "norm_corpus = [normalize(doc, stopwords) for doc in corpus]\n",
    "stemmed_corpus = [stemmatize(doc) for doc in norm_corpus]\n",
    "lemmed_corpus = [lemmatize(doc) for doc in norm_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set([word for doc in lemmed_corpus for word in doc])\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.6 Step 4: Count how many times each unique word appears in the documents (count matrix)\n",
    "\n",
    "- If a word appears frequently in all documents, it doesn't carry useful information\n",
    "- If a word only appears in one (or a few) documents, it is not good either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The count matrix:\n",
    "|-|character|say|age|bad|best|charles|dickens|foolishness|time|wisdom|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Doc1|1| 1| 0| 0| 1| 0| 0| 0| 1| 0|\n",
    "|Doc2|1| 1| 0| 1| 0| 0| 0| 0| 1| 0|\n",
    "|Doc3|1| 1| 1| 0| 0| 0| 0| 0| 0| 1|\n",
    "|Doc4|0| 0| 1| 0| 0| 1| 1| 1| 0| 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Word order does not matter!\n",
    "|-|age|bad|best|character|charles|dickens|foolishness|say|time|wisdom|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Doc1|0| 0| 1| 1| 0| 0| 0| 1| 1| 0|\n",
    "|Doc2|0| 1| 0| 1| 0| 0| 0| 1| 1| 0|\n",
    "|Doc3|1| 0| 0| 1| 0| 0| 0| 1| 0| 1|\n",
    "|Doc4|1| 0| 0| 0| 1| 1| 1| 0| 0| 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document vectors:\n",
    "- 'The character said: It was the best of times, ' => 'character said best time' => [0,0,1,1,0,0,0,1,1,0]\n",
    "- 'The character said: it was the worst of times, ' => 'character said worst time' => [0,1,0,1,0,0,01,1,0]\n",
    "- 'The character said: it was the Age of Wisdom, ' => 'character said age wisdom' => [1,0,0,1,0,0,0,1,0,1]\n",
    "- 'it was the Age of Foolishness. - Charles Dickens' => 'age foolishness charles dickens' => [1,0,0,0,1,1,1,0,0,0]\n",
    "___\n",
    "Key: ['age', 'bad', 'best', 'character', 'charles', 'dickens', 'foolishness', 'say', 'time', 'wisdom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.1 Bag of Words\n",
    "\n",
    "- A method to extract features from text documents\n",
    "- Can be used for training machine learning algorithms\n",
    "- Creates a vocabulary of unique words occurring in all documents\n",
    "- Represents a document as word counts, disregarding the order in which they appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### This problem has been solved already (like nearly all other programming problems you will come across)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.2 CountVectorizer will construct a bag of words matrix from a corpus of documents and it supports some of the pre-processing functions we coded before (lowercase, split into words/tokens, remove stopwords).\n",
    "\n",
    "#### If you don't need to do a lot of custom preprocessing to your corpus, you can directly input your corpus into CountVectorizer and let it do basic preprocessing for you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### For more granular control over pre-processing, do it outside of CountVectorizer, join your tokens back together, then run CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lemmed_corpus)\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "print(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "- Main goal of NLP in ML is to convert variable length documents into fixed length numbers\n",
    "- Text Normalization is the process of cleaning and processing raw text\n",
    "- Stemming and lemmatization are two attempts to reduce derived words to their bases\n",
    "- Bag of words counts how many times each unique word appears in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
