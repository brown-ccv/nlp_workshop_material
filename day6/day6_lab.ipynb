{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30)\n",
      "(200, 30)\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1\n",
      "10\n",
      "100\n",
      "1000\n",
      "[0.092, 0.092, 0.112, 0.152, 0.156, 0.15733333333333333, 0.16266666666666665]\n",
      "[0.064, 0.064, 0.072, 0.076, 0.068, 0.076, 0.076]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# day 6 lab\n",
    "#\n",
    "\n",
    "# step 1: load the 20 newsgroups dataset and other packages that you'll need (use code from yesterday)\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(1)\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      shuffle=True, \n",
    "                                      random_state=1, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "corpus = newsgroups_train.data[:1000]\n",
    "target = newsgroups_train.target[:1000]\n",
    "#print(target)\n",
    "#print(newsgroups_train.target_names)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                      shuffle=True, \n",
    "                                      random_state=1, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "corpus_test = newsgroups_test.data[:200]\n",
    "target_test = newsgroups_test.target[:200]\n",
    "\n",
    "# step 2: use the code you developed yesterday to convert the corpus into a feature matrix\n",
    "n_topics = 30\n",
    "t2_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "t2 = t2_vectorizer.fit_transform(corpus)\n",
    "t2_test = t2_vectorizer.transform(corpus_test)\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "X = LDA.fit_transform(t2)\n",
    "X_test = LDA.transform(t2_test)\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "# step 3: put together a ML pipeline using sklearn.linear_model.LogisticRegression\n",
    "# note: there is already a test in 20newsgroups so you only need to create train and CV sets\n",
    "# your hyperparameters are the four different preprocessing steps and C\n",
    "# check the LogisticRegression help to learn about C!\n",
    "# use accuracy as your evaluation metric (sklearn.metrics.accuracy_score)\n",
    "\n",
    "# step 3.1: split data into two parts (train and CV, 75% - 25%)\n",
    "X_train, X_CV, target_train, target_CV = train_test_split(X,target,test_size = 0.25,train_size =0.75)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_CV))\n",
    "#print(np.shape(target_train))\n",
    "#print(np.shape(target_CV))\n",
    "\n",
    "# step 3.2: hyperparameter values to try\n",
    "C_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# step 3.3: arrays for training and CV scores\n",
    "train_score = []\n",
    "CV_score = []\n",
    "\n",
    "# step 3.4: loop through the hyperparameters\n",
    "for C in C_list:\n",
    "    print(C)\n",
    "    Log_Reg = LogisticRegression(C = C,solver='lbfgs',multi_class='auto',max_iter=1000)\n",
    "    # fit the model to  determine the best parameters\n",
    "    Log_Reg.fit(X_train,target_train)\n",
    "    # predict the training set\n",
    "    y_pred_train = Log_Reg.predict(X_train)\n",
    "    # predict the CV set\n",
    "    y_pred_CV = Log_Reg.predict(X_CV)\n",
    "    \n",
    "    train_score.append(accuracy_score(target_train,y_pred_train))\n",
    "    CV_score.append(accuracy_score(target_CV,y_pred_CV))\n",
    "    \n",
    "# step 3.5: print the train and CV scores\n",
    "print(train_score)\n",
    "print(CV_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.105\n"
     ]
    }
   ],
   "source": [
    "# step 3.6: initialize the best classifier and report the test error\n",
    "Log_Reg = LogisticRegression(C = 1,solver='lbfgs',multi_class='auto',max_iter=1000)\n",
    "# fit the model to  determine the best parameters\n",
    "Log_Reg.fit(X_train,target_train)\n",
    "# predict the test set\n",
    "y_pred = Log_Reg.predict(X_test)\n",
    "print(accuracy_score(target_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
