{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# day 7 lab\n",
    "#\n",
    "#\n",
    "# IMPROVE THE TEST ACCURACY SCORE AS MUCH AS YOU CAN USING YESTERDAY'S CODE!\n",
    "#\n",
    "# You can do this in several ways:\n",
    "# 1) Rewrite the ML pipeline to use RandomForestClassifier instead of LogisticRegression.\n",
    "#    The hyperparameters in a random forest are n_estimators and max_depth.\n",
    "# 2) Improve the preprocessing by removing words with numbers and special characters in them. \n",
    "# 3) Check out the help of TfidfVectorizer and adjust some of its parameters. \n",
    "# 4) Treat the number of topics in LDA as a hyperparameter.\n",
    "# 5) Be creative! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2889)\n",
      "(200, 2889)\n",
      "[[0.06266667 0.07733333 0.168      0.32666667 0.58133333]\n",
      " [0.10533333 0.15866667 0.30666667 0.51333333 0.79066667]\n",
      " [0.16133333 0.28       0.49466667 0.74666667 0.95866667]\n",
      " [0.25333333 0.42       0.68133333 0.88666667 0.97066667]\n",
      " [0.25866667 0.488      0.77866667 0.93733333 0.97333333]\n",
      " [0.264      0.49066667 0.80666667 0.94933333 0.97466667]\n",
      " [0.27866667 0.49333333 0.804      0.948      0.97466667]]\n",
      "[[0.052 0.06  0.104 0.2   0.26 ]\n",
      " [0.052 0.08  0.16  0.252 0.284]\n",
      " [0.072 0.18  0.3   0.396 0.388]\n",
      " [0.164 0.3   0.428 0.448 0.428]\n",
      " [0.172 0.34  0.46  0.504 0.488]\n",
      " [0.188 0.336 0.488 0.516 0.516]\n",
      " [0.216 0.348 0.468 0.52  0.5  ]]\n"
     ]
    }
   ],
   "source": [
    "# step 1: load the 20 newsgroups dataset and other packages that you'll need\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      shuffle=True, \n",
    "                                      random_state=1, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "corpus = newsgroups_train.data[:1000]\n",
    "target = newsgroups_train.target[:1000]\n",
    "#print(target)\n",
    "#print(newsgroups_train.target_names)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                      shuffle=True, \n",
    "                                      random_state=1, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "corpus_test = newsgroups_test.data[:200]\n",
    "target_test = newsgroups_test.target[:200]\n",
    "\n",
    "# # preprocess corpus and test corpus\n",
    "# def preprocess(docs):\n",
    "#     docs = [doc.lower() for doc in docs]\n",
    "#     docs = [re.sub(r'[^\\w\\s]', '', doc) for doc in docs]\n",
    "#     new_docs = []\n",
    "#     ps = PorterStemmer()\n",
    "#     for doc in docs:\n",
    "#         stemmed = [ps.stem(word) for word in doc.split()]\n",
    "#         new_docs.append(' '.join(stemmed))\n",
    "#     return new_docs\n",
    "\n",
    "# corpus = preprocess(corpus)\n",
    "# corpus_test = preprocess(corpus_test)\n",
    "\n",
    "# step 2: use the code you developed yesterday to convert the corpus into a feature matrix\n",
    "t2_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english',min_df=5)\n",
    "X = t2_vectorizer.fit_transform(corpus)\n",
    "X_test = t2_vectorizer.transform(corpus_test)\n",
    "\n",
    "# print(np.shape(t2))\n",
    "# print(np.shape(t2_test))\n",
    "\n",
    "# n_topics = 30\n",
    "# LDA = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "# X = LDA.fit_transform(t2)\n",
    "# X_test = LDA.transform(t2_test)\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "# step 3: put together a ML pipeline using sklearn.linear_model.LogisticRegression\n",
    "# note: there is already a test in 20newsgroups so you only need to create train and CV sets\n",
    "# your hyperparameters are the four different preprocessing steps and C\n",
    "# check the LogisticRegression help to learn about C!\n",
    "# use accuracy as your evaluation metric (sklearn.metrics.accuracy_score)\n",
    "\n",
    "# step 3.1: split data into two parts (train and CV, 75% - 25%)\n",
    "X_train, X_CV, target_train, target_CV = train_test_split(X,target,test_size = 0.25,train_size =0.75)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_CV))\n",
    "#print(np.shape(target_train))\n",
    "#print(np.shape(target_CV))\n",
    "\n",
    "n_estimators = [1,3,10,30,100,300,1000]\n",
    "max_depth = [1,3,10,30,100]\n",
    "\n",
    "# arrays for training and test scores\n",
    "train_score = np.zeros([len(n_estimators),len(max_depth)])\n",
    "CV_score = np.zeros([len(n_estimators),len(max_depth)])\n",
    "\n",
    "# loop through the combinations of hyperparameters\n",
    "for n in range(len(n_estimators)):\n",
    "    for m in range(len(max_depth)):\n",
    "        # initialize the classifier\n",
    "        clf = RandomForestClassifier(n_estimators = n_estimators[n],max_depth=max_depth[m],random_state=10)\n",
    "        # fit the training data\n",
    "        clf.fit(X_train,target_train)\n",
    "        # predict the training data\n",
    "        y_p_train = clf.predict(X_train)\n",
    "        # predict the test data\n",
    "        y_p_CV = clf.predict(X_CV)\n",
    "        \n",
    "        # measure classifier performance with accuracy: what fraction of points are correctly classified?\n",
    "        train_score[n,m] = accuracy_score(target_train,y_p_train)\n",
    "        CV_score[n,m] = accuracy_score(target_CV,y_p_CV)\n",
    "\n",
    "# step 3.5: print the train and CV scores\n",
    "print(train_score)\n",
    "print(CV_score)\n",
    "# check CV_score to figure out which hyperparameter value to use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.465\n"
     ]
    }
   ],
   "source": [
    "# step 3.6: initialize the best classifier and report the test error\n",
    "clf = RandomForestClassifier(n_estimators = 1000,max_depth=30,random_state=10)\n",
    "# fit the model to  determine the best parameters\n",
    "clf.fit(X_train,target_train)\n",
    "# predict the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(target_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
