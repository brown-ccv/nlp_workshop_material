{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Intro to Natural Language Processing (NLP)\n",
    "#### By the end of this day, you'll be able to \n",
    "- explain the differences between bag of words and n-grams\n",
    "- apply the TFIDF transformation\n",
    "- explain the difference between topic modeling and LDA\n",
    "- run through a simple NLP pipeline, from cleaning and vectorizing text to mining topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1 N-grams\n",
    "- an ngram is a contiguous sequence of n items from a given sample of text or speech\n",
    "- the items can be phonemes, syllables, letters, words, or base pairs (according to the application)\n",
    "\n",
    "- we can generalize bag of words to phrases of *n* words\n",
    "- bag of words is a unigram representation of text\n",
    "- we can have unigrams, bigrams, 3-grams, 4-grams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Our corpus:\n",
    "- It was the best of times, it was the worst of times, it was the Age of Wisdom, it was the Age of Foolishness,\n",
    "\n",
    "#### Bigrams\n",
    "['it was', 'was the', 'the best', 'best of', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1 \n",
    "## Prepare the bigrams for this corpus (be sure to normalize/lemmatize the corpus first)\n",
    "- It was the best of times,\n",
    "- it was the worst of times,\n",
    "- it was the Age of Wisdom,\n",
    "- it was the Age of Foolishness,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "import re\n",
    "import contractions\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def normalize(doc, stopwords):\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    contractions.fix(doc)\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    p = inflect.engine()\n",
    "    words = [p.number_to_words(word) for word in words if word.isdigit()] + \\\n",
    "            [word for word in words if word.isdigit() == False]\n",
    "    words = [i for i in words if not i in stopwords]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    tagged_doc = nltk.pos_tag(doc)\n",
    "    lemmas = []\n",
    "    for word, tag in tagged_doc:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            lemma = lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag) \n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ngramize(doc, n):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    bigrams = list(ngrams(tokens, n))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['character say best time', 'character say bad time', 'character say age wisdom', 'age foolishness charles dickens']\n",
      "\n",
      "[[('character', 'say'), ('say', 'best'), ('best', 'time')], [('character', 'say'), ('say', 'bad'), ('bad', 'time')], [('character', 'say'), ('say', 'age'), ('age', 'wisdom')], [('age', 'foolishness'), ('foolishness', 'charles'), ('charles', 'dickens')]]\n"
     ]
    }
   ],
   "source": [
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "norm_corpus = [normalize(doc, sw) for doc in corpus]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmed_corpus  = [lemmatize(doc) for doc in norm_corpus]\n",
    "\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "print(clean_corpus)\n",
    "print()\n",
    "\n",
    "bigrams = [ngramize(doc, 2) for doc in clean_corpus]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.2 Bag of words vs. N-grams\n",
    "\n",
    "- bag of words is simple, but it is computationally inefficient\n",
    "- n-grams can create an even larger count matrix\n",
    "- n > 3 is rarely used\n",
    "- corpus of 1 billion ($10^9$) words contains roughly $10^5$ 1-grams, $3 \\times 10^5$ 2-grams, over $10^6$ 3-grams roughly\n",
    "- One counter-example: a large n can reveal plagarism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2 \n",
    "\n",
    "From `clean_corpus`, construct the bi-grams bag of words matrix in the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age foolishness', 'age wisdom', 'bad time', 'best time', 'character say', 'charles dickens', 'foolishness charles', 'say age', 'say bad', 'say best']\n",
      "[[0 0 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 0 1 0]\n",
      " [0 1 0 0 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.3 Towards the TFIDF transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.3.1 Another look at the count matrix\n",
    "\n",
    "|c|term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|\n",
    "\n",
    "- c is our count matrix\n",
    "- c_ij = number of times term_j appears in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.3.2 The term count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "\n",
    "- T is the term count vector\n",
    "- T_i is the number of terms in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.3.3 The document count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "|__<font color='blue'>D</font>__|__<font color='blue'>sum_i(c_i1 > 0)</font>__|__<font color='blue'>sum_i(c_i2 > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_ij > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_im > 0)</font>__||\n",
    "\n",
    "- D is the document count vector\n",
    "- D_j is the number of documents that contain term_j at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "## Calculate the term count and document counts of the following count matrix:\n",
    "\n",
    "|-|term_1|term_2|term_3|term_4|term_5|term_6|term_7|term_8|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc1__|2|3|1|0|3|3|1|2|\n",
    "|__Doc2__|1|3|1|0|0|0|3|2|\n",
    "|__Doc3__|1|0|1|2|0|1|0|1|\n",
    "|__Doc4__|0|2|1|3|3|0|0|3|\n",
    "|__Doc5__|2|2|2|0|1|0|3|2|\n",
    "|__Doc6__|1|0|3|3|1|2|3|1|\n",
    "|__Doc7__|2|2|0|2|0|2|3|2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Solution\n",
    "- T = [15, 10, 6, 12, 12, 14, 13]\n",
    "- D = [6, 5, 6, 4, 4, 4, 5, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.4 The TFIDF transformation\n",
    "\n",
    "#### We need to normalize or de-bias the count matrix!\n",
    "- Some documents are shorter, others are longer => there is a bias towards longer documents\n",
    "- Some terms appear in most of the documents => there is bias towards frequent terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.4.1 TFIDF - Term Frequency times Inverse Document Frequency\n",
    "- number of documents: n\n",
    "- term frequency: C_ij/T_i (frequency of word in a document)\n",
    "- document frequency: D_j/n (rank of the word for its relevancy in the corpus)\n",
    "- inverse document frequency (textbook): ln(n/D_j)\n",
    "- inverse document frequency (scikit learn): ln[(n+1)/(D_j+1)]+1\n",
    "    - adding 1 to numerator and denominator prevents zero divisions (as if extra document was seen containing every term in the corpus exactly once)\n",
    "    - adding 1 to idf is that terms with zero idf (terms that occur in all documents in a training set) will not be entirely ignored\n",
    "- tfidf = tf * idf\n",
    "### <center>W_ij = c_ij / T_i * ln(D_j / n)</center>\n",
    "#### <center>or</center>\n",
    "### <center>W_ij = c_ij / T_i * ln[(D_j + 1)/(n + 1)]+1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### W_ij = c_ij / T_i * ln[(D_j + 1)/(n + 1)]+1\n",
    "\n",
    "\n",
    "|__W__ |term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11/T_1 * ln[(D_1+1)/(n+1)]+1|c_12/T_1 * ln[(D_2+1)/n+1)]+1|...|c_1j/T_1 * ln[(D_j+1)/n+1)]+1|...|c_1m/T_1 * ln[(D_m+1)/n+1)]+1|\n",
    "|__Doc_2__|c_21/T_2 * ln[(D_1+1)/(n+1)]+1|c_22/T_2 * ln[(D_2+1)/n+1)]+1|...|c_2j/T_2 * ln[(D_j+1)/n+1)]+1|...|c_2m/T_2 * ln[(D_m+1)/n+1)]+1|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|c_i1/T_i * ln[(D_1+1)/(n+1)]+1|c_i2/T_i * ln[(D_2+1)/n+1)]+1|...|c_ij/T_i * ln[(D_j+1)/n+1)]+1|...|c_im/T_i * ln[(D_m+1)/n+1)]+1|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|c_n1/T_n * ln[(D_1+1)/(n+1)]+1|c_n2/T_n * ln[(D_2+1)/n+1)]+1|...|c_nj/T_n * ln[(D_j+1)/n+1)]+1|...|c_nm/T_n * ln[(D_m+1)/n+1)]+1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### then, normalize the weights so they are between 0 and 1 (L2)\n",
    "### TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 4\n",
    "### Calculate the TFIDF matrix for the following count matrix:\n",
    "\n",
    "\n",
    "|c |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0|1|1|0|1|1|<font color='red'>4</font>|\n",
    "|__Doc_2__|0|2|1|0|1|1|<font color='red'>5</font>|\n",
    "|__Doc_3__|1|0|1|1|1|1|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### First...\n",
    "## Calculate W_ij = c_ij / T_i * ln[(D_j + 1)/(n + 1)]+1\n",
    "|W |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0/4*ln(4/2)+1|1/4*ln(4/3)+1|1/4*ln(4/4)+1|0/4*ln(4/2)+1|1/4*ln(4/4)+1|1/4*ln(4/4)+1|<font color='red'>4</font>|\n",
    "|__Doc_2__|0/5*ln(4/2)+1|2/5*ln(4/3)+1|1/5*ln(4/4)+1|0/5*ln(4/2)+1|1/5*ln(4/4)+1|1/5*ln(4/4)+1|<font color='red'>5</font>|\n",
    "|__Doc_3__|1/5*ln(4/2)+1|0/5*ln(4/3)+1|1/5*ln(4/4)+1|1/5*ln(4/2)+1|1/5*ln(4/4)+1|1/5*ln(4/4)+1|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||\n",
    "\n",
    "|W |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0|0.3219|0.25|0|0.25|0.25|<font color='red'>4</font>|\n",
    "|__Doc_2__|0|0.51507|0.2|0|0.2|0.2|<font color='red'>5</font>|\n",
    "|__Doc_3__|0.3386|0|0.2|0.33863|0.2|0.2|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Then...\n",
    "\n",
    "## TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) ) \n",
    "normalize each row so weights are between 0 and 1\n",
    "\n",
    "$\\sqrt{ 0.3219^2+0.25^2+0.25^2+0.25^2 } = .539555$\n",
    "\n",
    "$\\sqrt{ 0.51507^2+0.2^2+0.2^2+0.2^2 } = .620723$\n",
    "\n",
    "$\\sqrt{ 0.3386^2+0.2^2+0.33863^2+0.2^2+0.2^2 } = .591033$\n",
    "\n",
    "|TFIDF |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0|0.59662724|0.46333427|0|0.46333427|0.46333427|<font color='red'>4</font>|\n",
    "|__Doc_2__|0|0.82979177|0.32220367|0|0.32220367|0.32220367|<font color='red'>5</font>|\n",
    "|__Doc_3__|0.57292883|0|0.338381|0.57292883|0.338381|0.338381|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.4.2 TFIDF Transformation in Corpus Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'is', 'paper', 'the', 'this']\n",
      "[[0 1 1 0 1 1]\n",
      " [0 2 1 0 1 1]\n",
      " [1 0 1 1 1 1]]\n",
      "[[0.         0.59662724 0.46333427 0.         0.46333427 0.46333427]\n",
      " [0.         0.82979177 0.32220367 0.         0.32220367 0.32220367]\n",
      " [0.57292883 0.         0.338381   0.57292883 0.338381   0.338381  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['This is the document.',\n",
    "          'This document is the document.',\n",
    "          'And this is the paper.']\n",
    "\n",
    "c_vectorizer = CountVectorizer()\n",
    "X = c_vectorizer.fit_transform(corpus)\n",
    "print(c_vectorizer.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "t_vectorizer = TfidfVectorizer()\n",
    "tfidf = t_vectorizer.fit_transform(corpus)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "norm_corpus = [normalize(doc) for doc in corpus]\n",
    "lemmed_corpus = [lemmatize(doc) for doc in norm_corpus]\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'bad', 'best', 'character', 'charles', 'dickens', 'foolishness', 'say', 'time', 'wisdom']\n",
      "[[0.         0.         0.64065543 0.40892206 0.         0.\n",
      "  0.         0.40892206 0.5051001  0.        ]\n",
      " [0.         0.64065543 0.         0.40892206 0.         0.\n",
      "  0.         0.40892206 0.5051001  0.        ]\n",
      " [0.5051001  0.         0.         0.40892206 0.         0.\n",
      "  0.         0.40892206 0.         0.64065543]\n",
      " [0.41428875 0.         0.         0.         0.52547275 0.52547275\n",
      "  0.52547275 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# the bigram TFIDF matrix\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.5 Topic Models\n",
    "\n",
    "- A type of statistical model for discovering the abstract “topics” that occur in a collection of documents\n",
    "- Can think of it as a form of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.5.1 Latent Dirichlet Allocation (LDA)\n",
    "- A flavor of topic modeling that can be used to classify text in a document to a particular topic. \n",
    "- The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document\n",
    "- Very popular since its inception in 2003 by David Blei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TODO:\n",
    "\n",
    "#### A Toy Example... \n",
    "- Document 1: I had a peanut butter sandwich for breakfast.\n",
    "- Document 2: I like to eat almonds, peanuts and walnuts.\n",
    "- Document 3: My neighbor got a little dog yesterday.\n",
    "- Document 4: Cats and dogs are mortal enemies.\n",
    "- Document 5: You mustn’t feed peanuts to your dog.\n",
    "\n",
    "```\n",
    "Topic 1: 30% peanuts, 15% almonds, 10% breakfast… (you can interpret that this topic deals with food)\n",
    "Topic 2: 20% dogs, 10% cats, 5% peanuts… ( you can interpret that this topic deals with pets or animals)\n",
    "\n",
    "Documents 1 and 2: 100% Topic 1\n",
    "Documents 3 and 4: 100% Topic 2\n",
    "Document 5: 70% Topic 1, 30% Topic 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.5.3 Example LDA Code Using `scikit learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ABC News Headlines Corpus from Kaggle\n",
    "\n",
    "Format: CSV\n",
    "\n",
    "- publish_date: Date of publishing for the article in yyyyMMdd format\n",
    "- headline_text: Text of the headline in Ascii, English, lowercase\n",
    "- Start Date: 2003-02-19 End Date: 2017-12-31\n",
    "- Total Records: 1,103,663\n",
    "\n",
    "Rohit Kulkarni (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: https://www.kaggle.com/therohk/million-headlines/downloads/million-headlines.zip/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation', 'a g calls for infrastructure protection summit', 'air nz staff in aust strike for pay rise', 'air nz strike to affect australian travellers']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./abcnews-date-text.csv', error_bad_lines=False)\n",
    "corpus = list(data['headline_text'])\n",
    "corpus = corpus[:1000]\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=30, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# set some parameters\n",
    "n_topics = 30\n",
    "n_top_words = 10\n",
    "ngram = 2\n",
    "\n",
    "# prepare the corpus\n",
    "norm_corpus = [normalize(doc, sw) for doc in corpus]\n",
    "lemmed_corpus = [lemmatize(doc) for doc in norm_corpus]\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "\n",
    "# model the cleaned corpus\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0).fit(X)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LatentDirichletAllocation in module sklearn.decomposition.online_lda:\n",
      "\n",
      "class LatentDirichletAllocation(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  LatentDirichletAllocation(n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None, n_topics=None)\n",
      " |  \n",
      " |  Latent Dirichlet Allocation with online variational Bayes algorithm\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, optional (default=10)\n",
      " |      Number of topics.\n",
      " |  \n",
      " |  doc_topic_prior : float, optional (default=None)\n",
      " |      Prior of document topic distribution `theta`. If the value is None,\n",
      " |      defaults to `1 / n_components`.\n",
      " |      In the literature, this is called `alpha`.\n",
      " |  \n",
      " |  topic_word_prior : float, optional (default=None)\n",
      " |      Prior of topic word distribution `beta`. If the value is None, defaults\n",
      " |      to `1 / n_components`.\n",
      " |      In the literature, this is called `eta`.\n",
      " |  \n",
      " |  learning_method : 'batch' | 'online', default='online'\n",
      " |      Method used to update `_component`. Only used in `fit` method.\n",
      " |      In general, if the data size is large, the online update will be much\n",
      " |      faster than the batch update.\n",
      " |      The default learning method is going to be changed to 'batch' in the\n",
      " |      0.20 release.\n",
      " |      Valid options::\n",
      " |  \n",
      " |          'batch': Batch variational Bayes method. Use all training data in\n",
      " |              each EM update.\n",
      " |              Old `components_` will be overwritten in each iteration.\n",
      " |          'online': Online variational Bayes method. In each EM update, use\n",
      " |              mini-batch of training data to update the ``components_``\n",
      " |              variable incrementally. The learning rate is controlled by the\n",
      " |              ``learning_decay`` and the ``learning_offset`` parameters.\n",
      " |  \n",
      " |  learning_decay : float, optional (default=0.7)\n",
      " |      It is a parameter that control learning rate in the online learning\n",
      " |      method. The value should be set between (0.5, 1.0] to guarantee\n",
      " |      asymptotic convergence. When the value is 0.0 and batch_size is\n",
      " |      ``n_samples``, the update method is same as batch learning. In the\n",
      " |      literature, this is called kappa.\n",
      " |  \n",
      " |  learning_offset : float, optional (default=10.)\n",
      " |      A (positive) parameter that downweights early iterations in online\n",
      " |      learning.  It should be greater than 1.0. In the literature, this is\n",
      " |      called tau_0.\n",
      " |  \n",
      " |  max_iter : integer, optional (default=10)\n",
      " |      The maximum number of iterations.\n",
      " |  \n",
      " |  batch_size : int, optional (default=128)\n",
      " |      Number of documents to use in each EM iteration. Only used in online\n",
      " |      learning.\n",
      " |  \n",
      " |  evaluate_every : int, optional (default=0)\n",
      " |      How often to evaluate perplexity. Only used in `fit` method.\n",
      " |      set it to 0 or negative number to not evalute perplexity in\n",
      " |      training at all. Evaluating perplexity can help you check convergence\n",
      " |      in training process, but it will also increase total training time.\n",
      " |      Evaluating perplexity in every iteration might increase training time\n",
      " |      up to two-fold.\n",
      " |  \n",
      " |  total_samples : int, optional (default=1e6)\n",
      " |      Total number of documents. Only used in the `partial_fit` method.\n",
      " |  \n",
      " |  perp_tol : float, optional (default=1e-1)\n",
      " |      Perplexity tolerance in batch learning. Only used when\n",
      " |      ``evaluate_every`` is greater than 0.\n",
      " |  \n",
      " |  mean_change_tol : float, optional (default=1e-3)\n",
      " |      Stopping tolerance for updating document topic distribution in E-step.\n",
      " |  \n",
      " |  max_doc_update_iter : int (default=100)\n",
      " |      Max number of iterations for updating document topic distribution in\n",
      " |      the E-step.\n",
      " |  \n",
      " |  n_jobs : int, optional (default=1)\n",
      " |      The number of jobs to use in the E-step. If -1, all CPUs are used. For\n",
      " |      ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Verbosity level.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  n_topics : int, optional (default=None)\n",
      " |      This parameter has been renamed to n_components and will\n",
      " |      be removed in version 0.21.\n",
      " |      .. deprecated:: 0.19\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : array, [n_components, n_features]\n",
      " |      Variational parameters for topic word distribution. Since the complete\n",
      " |      conditional for topic word distribution is a Dirichlet,\n",
      " |      ``components_[i, j]`` can be viewed as pseudocount that represents the\n",
      " |      number of times word `j` was assigned to topic `i`.\n",
      " |      It can also be viewed as distribution over the words for each topic\n",
      " |      after normalization:\n",
      " |      ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n",
      " |  \n",
      " |  n_batch_iter_ : int\n",
      " |      Number of iterations of the EM step.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of passes over the dataset.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D. Hoffman,\n",
      " |      David M. Blei, Francis Bach, 2010\n",
      " |  \n",
      " |  [2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n",
      " |      Chong Wang, John Paisley, 2013\n",
      " |  \n",
      " |  [3] Matthew D. Hoffman's onlineldavb code. Link:\n",
      " |      http://matthewdhoffman.com//code/onlineldavb.tar\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LatentDirichletAllocation\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None, n_topics=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Learn model for the data X with variational Bayes method.\n",
      " |      \n",
      " |      When `learning_method` is 'online', use mini-batch update.\n",
      " |      Otherwise, use batch update.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online VB with Mini-Batch update.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False)\n",
      " |      Calculate approximate perplexity for data X.\n",
      " |      \n",
      " |      Perplexity is defined as exp(-1. * log-likelihood per word)\n",
      " |      \n",
      " |      .. versionchanged:: 0.19\n",
      " |         *doc_topic_distr* argument has been deprecated and is ignored\n",
      " |         because user no longer has access to unnormalized distribution\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, [n_samples, n_features]\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      doc_topic_distr : None or array, shape=(n_samples, n_components)\n",
      " |          Document topic distribution.\n",
      " |          This argument is deprecated and is currently being ignored.\n",
      " |      \n",
      " |          .. deprecated:: 0.19\n",
      " |      \n",
      " |      sub_sampling : bool\n",
      " |          Do sub-sampling or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Perplexity score.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate approximate log-likelihood as score.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Use approximate bound as score.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform data X according to the fitted model.\n",
      " |      \n",
      " |         .. versionchanged:: 0.18\n",
      " |            *doc_topic_distr* is now normalized\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc_topic_distr : shape=(n_samples, n_components)\n",
      " |          Document topic distribution for X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LatentDirichletAllocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03518048, 0.03494565, 0.03519915, ..., 0.03476496, 0.03479979,\n",
       "        0.03467066],\n",
       "       [0.03491272, 0.03461502, 0.0347583 , ..., 0.0349228 , 0.03526037,\n",
       "        0.03476048],\n",
       "       [0.03488117, 0.03493359, 0.62683389, ..., 0.03522469, 0.03471125,\n",
       "        0.03505534],\n",
       "       ...,\n",
       "       [0.03506836, 0.03486589, 0.03495162, ..., 0.03496106, 0.03486878,\n",
       "        0.03478744],\n",
       "       [0.0349356 , 0.03492402, 0.03498623, ..., 0.034816  , 0.03486217,\n",
       "        0.03478007],\n",
       "       [0.03476832, 0.03476624, 0.03476583, ..., 0.0352323 , 0.03477653,\n",
       "        0.03521766]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2313)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LDA\n",
    "- Pro: The number of topics, K, is one of the only tuneable parameters in the model, making it simple and easy to use\n",
    "- Con: Evaluation is subjective and requires subject matter expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.6 Other popular NLP packages\n",
    "\n",
    "- Spacy\n",
    "- Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "- Main goal of NLP in ML is to convert variable length documents into fixed length numbers\n",
    "- Stemming and lemmatization are two attempts to reduce derived words to their bases\n",
    "- Bag of words counts how many times each unique word appears in a document\n",
    "- n grams counts how many times unique phrases of length n appear in a document\n",
    "- n > 3 is rarely used\n",
    "- While n grams are simple to calculate, the count matrix is sparse and requires a lot of memory to store => computationally inefficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap\n",
    "- TFIDF stands for Term Frequency times Inverse Document Frequency\n",
    "- The goal of TFIDF is to de-bias the count matrix\n",
    "- W_ij = c_ij / T_i * log( D_j/n )\n",
    "- TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap\n",
    "- LDA is one flavor of many different topic modeling algorithms\n",
    "- It can be used to summarize what a large, prohibitively long corpus of documents is about\n",
    "- The number of topics, K, is one of the only tuneable parameters in the model making it easy to use\n",
    "- Evaluation is subjective and requires subject matter expertise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
