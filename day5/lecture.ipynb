{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Please go to https://ccv.jupyter.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we learned so far...\n",
    "- The main goal of NLP in machine learning\n",
    "- Text normalization\n",
    "- The differences between stemming and lemmatization\n",
    "- Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Intro to Natural Language Processing (NLP)\n",
    "#### By the end of this day, you'll be able to \n",
    "- explain the differences between bag of words and n-grams\n",
    "- apply the TFIDF transformation\n",
    "- explain the difference between topic modeling and LDA\n",
    "- run through a simple NLP pipeline, from cleaning and vectorizing text to mining topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.1 N-grams\n",
    "- an ngram is a contiguous sequence of n items from a given sample of text or speech\n",
    "- the items can be phonemes, syllables, letters, words, or base pairs (according to the application)\n",
    "\n",
    "- we can generalize bag of words to phrases of *n* words\n",
    "- bag of words is a unigram representation of text\n",
    "- we can have unigrams, bigrams, 3-grams, 4-grams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Our corpus:\n",
    "- It was the best of times, it was the worst of times, it was the Age of Wisdom, it was the Age of Foolishness,\n",
    "\n",
    "#### Bigrams\n",
    "['it was', 'was the', 'the best', 'best of', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "help(nltk.ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 1 \n",
    "## Prepare the bigrams for each document in this corpus \n",
    "- It was the best of times,\n",
    "- it was the worst of times,\n",
    "- it was the Age of Wisdom,\n",
    "- it was the Age of Foolishness,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2 Bag of words using n-grams\n",
    "\n",
    "- changing the unit of analysis from words to n-grams can help to encode some contextual information in NLP applications\n",
    "- `CountVectorizer()` has a built-in `ngrams_range` option for computing a count matrix with any number of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2.1 Bag of words vs. N-grams\n",
    "\n",
    "- bag of words is simple, but it is computationally inefficient\n",
    "- n-grams can create an even larger count matrix\n",
    "- n > 3 is rarely used\n",
    "- corpus of 1 billion ($10^9$) words contains roughly $10^5$ 1-grams, $3 \\times 10^5$ 2-grams, over $10^6$ 3-grams roughly\n",
    "- One counter-example: a large n can reveal plagarism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2 \n",
    "\n",
    "### From the corpus below, construct a count matrix with a vocabulary made up of both bigrams and trigrams. Print the vocabulary and the count array.\n",
    "\n",
    "```\n",
    "corpus = ['It was the best of times,',\n",
    "          'it was the worst of times,',\n",
    "          'it was the Age of Wisdom,',\n",
    "          'it was the Age of Foolishness,']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.3 Towards the TFIDF transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3.1 Another look at the count matrix\n",
    "\n",
    "|c|term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|\n",
    "\n",
    "- c is our count matrix\n",
    "- c_ij = number of times term_j appears in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3.2 The term count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "\n",
    "- T is the term count vector\n",
    "- T_i is the number of terms in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3.3 The document count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "|__<font color='blue'>D</font>__|__<font color='blue'>sum_i(c_i1 > 0)</font>__|__<font color='blue'>sum_i(c_i2 > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_ij > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_im > 0)</font>__||\n",
    "\n",
    "- D is the document count vector\n",
    "- D_j is the number of documents that contain term_j at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "## Calculate the term count and document counts of the following count matrix:\n",
    "\n",
    "|-|term_1|term_2|term_3|term_4|term_5|term_6|term_7|term_8|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc1__|2|3|1|0|3|3|1|2|\n",
    "|__Doc2__|1|3|1|0|0|0|3|2|\n",
    "|__Doc3__|1|0|1|2|0|1|0|1|\n",
    "|__Doc4__|0|2|1|3|3|0|0|3|\n",
    "|__Doc5__|2|2|2|0|1|0|3|2|\n",
    "|__Doc6__|1|0|3|3|1|2|3|1|\n",
    "|__Doc7__|2|2|0|2|0|2|3|2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.4 The TFIDF transformation\n",
    "\n",
    "#### We need to normalize or de-bias the count matrix!\n",
    "- Some documents are shorter, others are longer => there is a bias towards longer documents\n",
    "- Some terms appear in most of the documents => there is bias towards frequent terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.4.1 TFIDF - Term Frequency times Inverse Document Frequency\n",
    "- number of documents: n\n",
    "- term frequency: C_ij/T_i (frequency of word in a document)\n",
    "- document frequency: D_j/n (rank of the word for its relevancy in the corpus)\n",
    "- inverse document frequency (textbook): ln(n/D_j)\n",
    "- inverse document frequency (scikit learn): ln[(n+1)/(D_j+1)]+1\n",
    "    - adding 1 to numerator and denominator prevents zero divisions (as if extra document was seen containing every term in the corpus exactly once)\n",
    "    - adding 1 to idf is that terms with zero idf (terms that occur in all documents in a training set) will not be entirely ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- W_ij = tf * idf\n",
    "### <center>W_ij = c_ij / T_i * ln(D_j / n)</center>\n",
    "#### <center>or</center>\n",
    "### <center>W_ij = (c_ij / T_i) * (ln[(D_j + 1)/(n + 1)]+1)</center>\n",
    "\n",
    "\n",
    "- normalize the weights so the weights are between 0 and 1 for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### W_ij = (c_ij / T_i)*(ln[(D_j + 1)/(n + 1)]+1)\n",
    "\n",
    "\n",
    "|__W__ |term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|( c_11/T_1 )*( ln[(D_1+1)/(n+1)]+1 )|( c_12/T_1 )*( ln[(D_2+1)/n+1)]+1 )|...|( c_1j/T_1 )*( ln[(D_j+1)/n+1)]+1 )|...|( c_1m/T_1 )*( ln[(D_m+1)/n+1)]+1 )|\n",
    "|__Doc_2__|( c_21/T_2 )*( ln[(D_1+1)/(n+1)]+1 )|( c_22/T_2 )*( ln[(D_2+1)/n+1)]+1 )|...|( c_2j/T_2 )*( ln[(D_j+1)/n+1)]+1 )|...|( c_2m/T_2 )*( ln[(D_m+1)/n+1)]+1 )|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|( c_i1/T_i )*( ln[(D_1+1)/(n+1)]+1 )|( c_i2/T_i )*( ln[(D_2+1)/n+1)]+1 )|...|( c_ij/T_i )*( ln[(D_j+1)/n+1)]+1 )|...|( c_im/T_i )*( ln[(D_m+1)/n+1)]+1 )|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|( c_n1/T_n )*( ln[(D_1+1)/(n+1)]+1 )|( c_n2/T_n )*( ln[(D_2+1)/n+1)]+1 )|...|( c_nj/T_n )*( ln[(D_j+1)/n+1)]+1 )|...|( c_nm/T_n )*( ln[(D_m+1)/n+1)]+1 )|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### then, normalize the weights so they are between 0 and 1 (L2)\n",
    "### TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 4\n",
    "### Calculate the TFIDF matrix for the following count matrix:\n",
    "\n",
    "\n",
    "|c |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0|1|1|0|1|1|<font color='red'>4</font>|\n",
    "|__Doc_2__|0|2|1|0|1|1|<font color='red'>5</font>|\n",
    "|__Doc_3__|1|0|1|1|1|1|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.4.2 TFIDF Transformation in Corpus Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['This is the document.',\n",
    "          'This document is the document.',\n",
    "          'And this is the paper.']\n",
    "\n",
    "c_vectorizer = CountVectorizer()\n",
    "X = c_vectorizer.fit_transform(corpus)\n",
    "print(c_vectorizer.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "t_vectorizer = TfidfVectorizer()\n",
    "tfidf = t_vectorizer.fit_transform(corpus)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The character said: It was the best of times, ', \n",
    "          'The character said: it was the worst of times, ', \n",
    "          'The character said: it was the Age of Wisdom, ', \n",
    "          'it was the Age of Foolishness. - Charles Dickens']\n",
    "norm_corpus = [normalize(doc) for doc in corpus]\n",
    "lemmed_corpus = [lemmatize(doc) for doc in norm_corpus]\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the bigram TFIDF matrix\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.5 Topic Models\n",
    "\n",
    "- A type of statistical model for discovering the abstract “topics” that occur in a collection of documents\n",
    "- Can think of it as a form of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.5.1 Latent Dirichlet Allocation (LDA)\n",
    "- A flavor of topic modeling that can be used to classify text in a document to a particular topic. \n",
    "- The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document\n",
    "- Very popular since its inception in 2003 by David Blei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![title](./lda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.5.3 Example LDA Code Using `scikit learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ABC News Headlines Corpus from Kaggle\n",
    "\n",
    "Format: CSV\n",
    "\n",
    "- publish_date: Date of publishing for the article in yyyyMMdd format\n",
    "- headline_text: Text of the headline in Ascii, English, lowercase\n",
    "- Start Date: 2003-02-19 End Date: 2017-12-31\n",
    "- Total Records: 1,103,663\n",
    "\n",
    "Rohit Kulkarni (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: https://www.kaggle.com/therohk/million-headlines/downloads/million-headlines.zip/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./abcnews-date-text.csv', error_bad_lines=False)\n",
    "corpus = list(data['headline_text'])\n",
    "corpus = corpus[:10000]\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# set some parameters\n",
    "n_topics = 30\n",
    "n_top_words = 10\n",
    "ngram = 2\n",
    "\n",
    "# prepare the corpus\n",
    "norm_corpus = [normalize(doc) for doc in corpus]\n",
    "lemmed_corpus = [lemmatize(doc) for doc in norm_corpus]\n",
    "clean_corpus = [' '.join(doc) for doc in lemmed_corpus]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(clean_corpus)\n",
    "\n",
    "# model the cleaned corpus\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0).fit(X)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "help(LatentDirichletAllocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the topic word weights\n",
    "print(lda.components_)\n",
    "print(lda_components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the document topic weights\n",
    "lda_X = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0).fit_transform(X)\n",
    "print(lda_X)\n",
    "print(lda_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LDA\n",
    "- Pro: The number of topics, K, is one of the only tuneable parameters in the model, making it simple and easy to use\n",
    "- Con: Evaluation is subjective and requires subject matter expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic ' + str(topic_idx) + ':')\n",
    "        print('|'.join([feature_names[i] for i in np.argsort(topic)[:-n_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_feature_names = vectorizer.get_feature_names()\n",
    "display_topics(lda, X_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.6 Other popular NLP packages\n",
    "\n",
    "- Spacy\n",
    "- Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "- Main goal of NLP in ML is to convert variable length documents into fixed length numbers\n",
    "- Stemming and lemmatization are two attempts to reduce derived words to their bases\n",
    "- Bag of words counts how many times each unique word appears in a document\n",
    "- n grams counts how many times unique phrases of length n appear in a document\n",
    "- n > 3 is rarely used\n",
    "- While n grams are simple to calculate, the count matrix is sparse and requires a lot of memory to store => computationally inefficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap\n",
    "- TFIDF stands for Term Frequency times Inverse Document Frequency\n",
    "- The goal of TFIDF is to de-bias the count matrix\n",
    "- W_ij = c_ij / T_i * log( D_j/n )\n",
    "- TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap\n",
    "- LDA is one flavor of many different topic modeling algorithms\n",
    "- It can be used to summarize what a large, prohibitively long corpus of documents is about\n",
    "- It builds a topic per document model and words per topic model, modeled as Dirichlet distributions\n",
    "- The number of topics, K, is one of the only tuneable parameters in the model\n",
    "- Evaluation is subjective and requires subject matter expertise "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
