{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Please go to https://ccv.jupyter.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we learned so far...\n",
    "- The main goal of NLP in machine learning\n",
    "- Text normalization\n",
    "- The differences between stemming and lemmatization\n",
    "- Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Intro to Natural Language Processing (NLP)\n",
    "#### By the end of this day, you'll be able to \n",
    "- explain the differences between bag of words and n-grams\n",
    "- apply the TFIDF transformation\n",
    "- explain the difference between topic modeling and LDA\n",
    "- run through a simple NLP pipeline, from cleaning and vectorizing text to mining topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.1 N-grams\n",
    "- an ngram is a contiguous sequence of n items from a given sample of text\n",
    "- the items can be syllables, letters, words, or base pairs (according to the application)\n",
    "\n",
    "- we can generalize bag of words to phrases of *n* words\n",
    "- bag of words is a unigram representation of text\n",
    "- we can have unigrams, bigrams, 3-grams, 4-grams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A document:\n",
    "- 'It was the best of times'\n",
    "\n",
    "#### Bigrams:   ['it was', 'was the', 'the best', 'best of', 'of times']\n",
    "#### Trigrams:   ['it was the', 'was the best', 'the best of', 'best of times']\n",
    "#### 4-grams: ['it was the best', 'was the best of', 'the best of times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1.2 Bag of words using n-grams\n",
    "\n",
    "- changing the unit of analysis from words to n-grams can help to encode some contextual information in NLP applications\n",
    "- `CountVectorizer()` has a built-in `ngrams_range` option for computing a count matrix with any number of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['It was the best of times,',\n",
    "          'it was the worst of times,',\n",
    "          'it was the Age of Wisdom,',\n",
    "          'it was the Age of Foolishness,']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "count_matrix = X.toarray()\n",
    "bigrams = vectorizer.get_feature_names()\n",
    "\n",
    "print(count_matrix)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Construct a count matrix with a vocabulary made up of both bigrams and trigrams. Print the vocabulary and the count array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "count_matrix = X.toarray()\n",
    "bigrams = vectorizer.get_feature_names()\n",
    "\n",
    "print(count_matrix)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "### Construct a count matrix with a vocabulary made up of trigrams. Print the vocabulary and the count array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1.3 Bag of words vs. N-grams\n",
    "\n",
    "- bag of words is simple, but it is computationally inefficient\n",
    "- n-grams can create an even larger count matrix\n",
    "- corpus of 1 billion ($ 10^9 $) words contains roughly $ 10^5 $ 1-grams, $ 3 \\times 10^5 $ 2-grams, over $ 10^6 $ 3-grams\n",
    "- n > 3 is rarely used\n",
    "- One counter-example: a large n can reveal plagarism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.2 Towards the TFIDF transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.1 Another look at the count matrix\n",
    "\n",
    "|c|term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|\n",
    "\n",
    "- c is our count matrix\n",
    "- c_ij = number of times term_j appears in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.2 The term count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "\n",
    "- T is the term count vector\n",
    "- T_i is the number of terms in document_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.3 The document count vector\n",
    "\n",
    "|-|term_1|term_2|...|term_j|...|term_m|__<font color='red'>T</font>__|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|c_11|c_12|...|c_1j|...|c_1m|__<font color='red'>sum_j(c_1j)</font>__|\n",
    "|__Doc_2__|c_21|c_22|...|c_2j|...|c_2m|__<font color='red'>sum_j(c_2j)</font>__|\n",
    "|__...__|...|...|...|...|...|...|<font color='red'>...</font>|\n",
    "|__Doc_i__|c_i1|c_i2|...|c_ij|...|c_im|__<font color='red'>sum_j(c_ij)</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>|\n",
    "|__Doc_n__|c_n1|c_n2|...|c_nj|...|c_nm|__<font color='red'>sum_j(c_nj)</font>__|\n",
    "|__<font color='blue'>D</font>__|__<font color='blue'>sum_i(c_i1 > 0)</font>__|__<font color='blue'>sum_i(c_i2 > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_ij > 0)</font>__|<font color='blue'>...</font>|__<font color='blue'>sum_i(c_im > 0)</font>__||\n",
    "\n",
    "- D is the document count vector\n",
    "- D_j is the number of documents that contain term_j at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "### Calculate (by hand) the term count vector and document count vector of the following count matrix:\n",
    "\n",
    "|-|term_1|term_2|term_3|term_4|term_5|term_6|term_7|term_8|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc1__|2|3|1|0|3|3|1|2|\n",
    "|__Doc2__|1|3|1|0|0|0|3|2|\n",
    "|__Doc3__|1|0|1|2|0|1|0|1|\n",
    "|__Doc4__|0|2|1|3|3|0|0|3|\n",
    "|__Doc5__|2|2|2|0|1|0|3|2|\n",
    "|__Doc6__|1|0|3|3|1|2|3|1|\n",
    "|__Doc7__|2|2|0|2|0|2|3|2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.3 The TFIDF transformation\n",
    "\n",
    "#### We need to normalize or de-bias the count matrix!\n",
    "- Some documents are shorter, others are longer => there is a bias towards longer documents\n",
    "- Some terms appear in most of the documents => there is bias towards frequent terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3.1 TFIDF - Term Frequency times Inverse Document Frequency\n",
    "\n",
    "1. Compute the weights, W_ij = tf * idf\n",
    "    - tf = C_ij/T_i (frequency of word in a document)\n",
    "    - idf = ln(n/D_j)\n",
    "    - df = D_j/n (rank the word for how many documents it shows up in)\n",
    "    - n = number of documents\n",
    "#### <center>W_ij = c_ij / T_i * ln(n / D_j)</center>\n",
    "\n",
    "2. Normalize the weights so the weights are between 0 and 1 for each document\n",
    "#### <center>TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) )</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### That was the textbook definition of TFIDF. The `sklearn` package defines TFIDF in a slightly more complicated way..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### <center>W_ij = (c_ij / T_i) * (ln[(n + 1)/(D_j + 1)]+1)</center>\n",
    "- idf(sklearn) = ln[(n+1)/(D_j+1)]+1\n",
    "    - adding 1 to numerator and denominator prevents zero divisions (as if extra document was seen containing every term in the corpus exactly once)\n",
    "    - adding 1 to idf prevents terms with zero idf from being ignored (terms that occur in all documents have zero idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.3.2 Apply TFIDF to a count matrix\n",
    "\n",
    "\n",
    "|__W__ |term_1|term_2|...|term_j|...|term_m|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|( c_11/T_1 )*( ln[(n+1)/(D_1+1)]+1 )|( c_12/T_1 )*( ln[(n+1)/(D_2+1)]+1 )|...|( c_1j/T_1 )*( ln[(n+1)/(D_j+1)]+1 )|...|( c_1m/T_1 )*( ln[(n+1)/(D_m+1)]+1 )|\n",
    "|__Doc_2__|( c_21/T_2 )*( ln[(n+1)/(D_1+1)]+1 )|( c_22/T_2 )*( ln[(n+1)/(D_2+1)]+1 )|...|( c_2j/T_2 )*( ln[(n+1)/(D_j+1)]+1 )|...|( c_2m/T_2 )*( ln[(n+1)/(D_m+1)]+1 )|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_i__|( c_i1/T_i )*( ln[(n+1)/(D_1+1)]+1 )|( c_i2/T_i )*( ln[(n+1)/(D_2+1)]+1 )|...|( c_ij/T_i )*( ln[(n+1)/(D_j+1)]+1 )|...|( c_im/T_i )*( ln[(n+1)/(D_m+1)]+1 )|\n",
    "|__...__|...|...|...|...|...|...|\n",
    "|__Doc_n__|( c_n1/T_n )*( ln[(n+1)/(D_1+1)]+1 )|( c_n2/T_n )*( ln[(n+1)/(D_2+1)]+1 )|...|( c_nj/T_n )*( ln[(n+1)/(D_j+1)]+1 )|...|( c_nm/T_n )*( ln[(n+1)/(D_m+1)]+1 )|\n",
    "\n",
    "#### then, normalize the weights so they are between 0 and 1 for each document\n",
    "TFIDF_ij = W_ij / sqrt( sum( W_ij^2 ) )\n",
    "\n",
    "TFIDF_11 = W_11 / sqrt(W_11 + W_12 + W_13 + W_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "### Calculate (by hand) the TFIDF matrix for Doc_1 of the following count matrix:\n",
    "\n",
    "\n",
    "|c |term_1|term_2|term_3|term_4|term_5|term_6|<font color='red'>T</font>|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__Doc_1__|0|1|1|0|1|1|<font color='red'>4</font>|\n",
    "|__Doc_2__|0|2|1|0|1|1|<font color='red'>5</font>|\n",
    "|__Doc_3__|1|0|1|1|1|1|<font color='red'>5</font>|\n",
    "|__<font color='blue'>D</font>__|<font color='blue'>1</font>|<font color='blue'>2</font>|<font color='blue'>3</font>|<font color='blue'>1</font>|<font color='blue'>3</font>|<font color='blue'>3</font>||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.4 TFIDF Transformation Using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['This is the document.',\n",
    "          'This document is the document.',\n",
    "          'And this is the paper.']\n",
    "\n",
    "t_vectorizer = TfidfVectorizer()\n",
    "X = t_vectorizer.fit_transform(corpus)\n",
    "words = t_vectorizer.get_feature_names()\n",
    "tfidf_matrix = X.toarray()\n",
    "print(words)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.5 Topic Models\n",
    "\n",
    "- A type of statistical model for discovering the abstract “topics” that occur in a collection of documents\n",
    "- Useful for summarizing a large corpus that is too big to be read by a human\n",
    "- Can think of it as a form of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.5.1 Latent Dirichlet Allocation (LDA)\n",
    "- A flavor of topic modeling; very popular since its inception in 2003 by David Blei\n",
    "- Models documents as a collection of topics; models topics as a collection of words\n",
    "- Assumptions\n",
    "    - documents are made up of multiple topics\n",
    "        - some topics are more present than others in a document\n",
    "    - topics are the ranked words in the corpus\n",
    "        - some words are more probable than others in a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![title](./lda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.5.2 Example LDA Code Using `scikit learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ABC News Headlines Corpus from Kaggle\n",
    "\n",
    "- publish_date: Date of publishing for the article in yyyyMMdd format\n",
    "- headline_text: Text of the headline in Ascii, English, lowercase\n",
    "- Start Date: 2003-02-19 End Date: 2017-12-31\n",
    "- Total Records: 1,103,663\n",
    "\n",
    "Rohit Kulkarni (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: https://www.kaggle.com/therohk/million-headlines/downloads/million-headlines.zip/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./abcnews-date-text.csv', error_bad_lines=False)\n",
    "print(data.head())\n",
    "\n",
    "corpus = list(data['headline_text'])\n",
    "corpus = corpus[:10000]\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# prepare the corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# initialize the LDA model with some parameters\n",
    "n_topics = 30\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get the topic word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topic_word_weights = lda.fit(X).components_\n",
    "print('the shape of the LDA output is ' + str(topic_word_weights.shape))\n",
    "print('there are ' + str(len(words)) + ' unique words in the corpus')\n",
    "print('there are ' + str(n_topics) + ' topics in our model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# print the topic word weights\n",
    "print(topic_word_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get the document topic weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the document topic weights\n",
    "doc_topic_weights = lda.fit_transform(X)\n",
    "print('there are ' + str(len(corpus)) + ' documents in the corpus')\n",
    "print(doc_topic_weights.shape)\n",
    "print(doc_topic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Display the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_topics(model, vocab, n_top_words=10):\n",
    "    \"\"\"\n",
    "    - argsort gives the indices of topic array to sort it\n",
    "    - the slice 0:-11:-1 gets the last 10 elements of the indices array, which can\n",
    "      then be used to access the top 10 occurring words in the topic\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic ' + str(topic_idx) + ':')\n",
    "        print('|'.join([vocab[i] for i in np.argsort(topic)[:-n_top_words-1:-1]]))\n",
    "        \n",
    "display_topics(lda, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pros and Cons of LDA\n",
    "- Pro: Simple to use, can get very far just tuning the number of topics\n",
    "- Con: Evaluation is subjective and requires subject matter expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.6 Other popular NLP packages\n",
    "\n",
    "- Spacy\n",
    "- Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "- n grams counts how many times unique phrases of length n appear in a document\n",
    "- n > 3 is rarely used\n",
    "- While n grams are simple to calculate, the count matrix is sparse and requires a lot of memory to store => computationally inefficient \n",
    "- The goal of TFIDF is to de-bias the count matrix\n",
    "- LDA is one flavor of many different topic modeling algorithms\n",
    "- It can be used to summarize what a large, prohibitively long corpus of documents is about\n",
    "- Evaluation is subjective and requires subject matter expertise "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
